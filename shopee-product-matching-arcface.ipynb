{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mathematical-being",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T05:17:08.609642Z",
     "iopub.status.busy": "2021-05-05T05:17:08.609012Z",
     "iopub.status.idle": "2021-05-05T05:17:14.966913Z",
     "shell.execute_reply": "2021-05-05T05:17:14.965905Z"
    },
    "papermill": {
     "duration": 6.377711,
     "end_time": "2021-05-05T05:17:14.967073",
     "exception": false,
     "start_time": "2021-05-05T05:17:08.589362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "billion-lambda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T05:17:14.991533Z",
     "iopub.status.busy": "2021-05-05T05:17:14.990639Z",
     "iopub.status.idle": "2021-05-05T05:17:18.218260Z",
     "shell.execute_reply": "2021-05-05T05:17:18.217779Z"
    },
    "papermill": {
     "duration": 3.242231,
     "end_time": "2021-05-05T05:17:18.218390",
     "exception": false,
     "start_time": "2021-05-05T05:17:14.976159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"use_inf_as_na\", True)\n",
    "pd.set_option(\"display.max_columns\", 9999)\n",
    "pd.set_option(\"display.max_rows\", 9999)\n",
    "pd.set_option('max_colwidth', 9999)\n",
    "INPUT = '/kaggle/input'\n",
    "DATA = f'{INPUT}/shopee-product-matching'\n",
    "OUTPUT = '/kaggle/working'\n",
    "RESOURCE_DIR = f'{INPUT}/shopee-product-matching-lib/kaggle-shopee-product-matching-1.0'\n",
    "sys.path.append(f'{INPUT}/sgcharts-ml/src')\n",
    "sys.path.append(f\"{INPUT}/sentence-transformers/sentence-transformers-1.0.4\")\n",
    "sys.path.append(f'{RESOURCE_DIR}/src')\n",
    "import mylib\n",
    "import scml\n",
    "from scml.nlp import strip_punctuation, to_ascii_str\n",
    "scml.seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fresh-series",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T05:17:18.242327Z",
     "iopub.status.busy": "2021-05-05T05:17:18.241537Z",
     "iopub.status.idle": "2021-05-05T05:17:18.243786Z",
     "shell.execute_reply": "2021-05-05T05:17:18.244162Z"
    },
    "papermill": {
     "duration": 0.017158,
     "end_time": "2021-05-05T05:17:18.244300",
     "exception": false,
     "start_time": "2021-05-05T05:17:18.227142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL = 'efficientnetb0'\n",
    "CONF = {\n",
    "    'efficientnetb0': {\n",
    "        'resolution': 224,\n",
    "        'output_size': 1280,\n",
    "    },\n",
    "    'efficientnetb1': {\n",
    "        'resolution': 240,\n",
    "        'output_size': 0,\n",
    "    },\n",
    "    'efficientnetb2': {\n",
    "        'resolution': 260,\n",
    "        'output_size': 1408,\n",
    "    },\n",
    "    'efficientnetb3': {\n",
    "        'resolution': 300,\n",
    "        'output_size': 1536,\n",
    "    },\n",
    "    'efficientnetb4': {\n",
    "        'resolution': 380,\n",
    "        'output_size': 1792,\n",
    "    },\n",
    "    'efficientnetb5': {\n",
    "        'resolution': 456,\n",
    "        'output_size': 2048,\n",
    "    },\n",
    "    'efficientnetb6': {\n",
    "        'resolution': 528,\n",
    "        'output_size': 2304,\n",
    "    },\n",
    "    'efficientnetb7': {\n",
    "        'resolution': 600,\n",
    "        'output_size': 2560,\n",
    "    },\n",
    "}\n",
    "INPUT_SHAPE = (CONF[MODEL]['resolution'], CONF[MODEL]['resolution'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "reflected-rehabilitation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T05:17:18.266429Z",
     "iopub.status.busy": "2021-05-05T05:17:18.265847Z",
     "iopub.status.idle": "2021-05-05T05:17:19.142998Z",
     "shell.execute_reply": "2021-05-05T05:17:19.143570Z"
    },
    "papermill": {
     "duration": 0.890952,
     "end_time": "2021-05-05T05:17:19.143746",
     "exception": false,
     "start_time": "2021-05-05T05:17:18.252794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_classes=11014\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34250 entries, 0 to 34249\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   posting_id   34250 non-null  object\n",
      " 1   image        34250 non-null  object\n",
      " 2   image_phash  34250 non-null  object\n",
      " 3   title        34250 non-null  object\n",
      " 4   label_group  34250 non-null  int64 \n",
      " 5   target       34250 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(f\"{DATA}/train.csv\", engine=\"c\", low_memory=False)\n",
    "train[\"target\"] = mylib.target_label(train)\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "train[\"label_group\"] = le.fit_transform(train['label_group'])\n",
    "n_classes=len(le.classes_)\n",
    "print(f\"n_classes={n_classes}\")\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stuck-mandate",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T05:17:19.174065Z",
     "iopub.status.busy": "2021-05-05T05:17:19.173403Z",
     "iopub.status.idle": "2021-05-05T05:17:19.183081Z",
     "shell.execute_reply": "2021-05-05T05:17:19.183490Z"
    },
    "papermill": {
     "duration": 0.029664,
     "end_time": "2021-05-05T05:17:19.183613",
     "exception": false,
     "start_time": "2021-05-05T05:17:19.153949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>666</td>\n",
       "      <td>[train_129225211, train_2278313361]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DOUBLE FOAM TAPE</td>\n",
       "      <td>7572</td>\n",
       "      <td>[train_3386243561, train_3423213080]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>6172</td>\n",
       "      <td>[train_2288590299, train_3803689425]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Campur - Leher Kancing (DPT001-00) Batik karakter Alhadi</td>\n",
       "      <td>10509</td>\n",
       "      <td>[train_2406599165, train_3342059966]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>9425</td>\n",
       "      <td>[train_3369186413, train_921438619]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                                                                                title  \\\n",
       "0                                                                           Paper Bag Victoria Secret   \n",
       "1                                        Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DOUBLE FOAM TAPE   \n",
       "2                                                         Maling TTS Canned Pork Luncheon Meat 397 gr   \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Campur - Leher Kancing (DPT001-00) Batik karakter Alhadi   \n",
       "4                                                                   Nescafe \\xc3\\x89clair Latte 220ml   \n",
       "\n",
       "   label_group                                target  \n",
       "0          666   [train_129225211, train_2278313361]  \n",
       "1         7572  [train_3386243561, train_3423213080]  \n",
       "2         6172  [train_2288590299, train_3803689425]  \n",
       "3        10509  [train_2406599165, train_3342059966]  \n",
       "4         9425   [train_3369186413, train_921438619]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "later-survival",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T05:17:19.210216Z",
     "iopub.status.busy": "2021-05-05T05:17:19.209497Z",
     "iopub.status.idle": "2021-05-05T05:17:19.212302Z",
     "shell.execute_reply": "2021-05-05T05:17:19.211903Z"
    },
    "papermill": {
     "duration": 0.019235,
     "end_time": "2021-05-05T05:17:19.212411",
     "exception": false,
     "start_time": "2021-05-05T05:17:19.193176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _data_gen(\n",
    "    dataframe,\n",
    "    directory,\n",
    "    target_size,\n",
    "    batch_size,\n",
    "    mode,\n",
    "    color_mode=\"rgb\",\n",
    "    class_mode=\"raw\",\n",
    "    x_col=\"image\",\n",
    "    y_col=\"label_group\"\n",
    "):\n",
    "    dtype = np.float32\n",
    "    rescale = 1./255\n",
    "    interpolation = \"nearest\"\n",
    "    data_format = \"channels_last\"\n",
    "    shuffle = False\n",
    "    idg = keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=rescale,\n",
    "        data_format=data_format,\n",
    "        dtype=dtype\n",
    "    )\n",
    "    if mode == \"train\":\n",
    "        shuffle = True\n",
    "        idg = keras.preprocessing.image.ImageDataGenerator(\n",
    "            #shear_range=0.2,\n",
    "            #zoom_range=0.2,\n",
    "            #horizontal_flip=True,\n",
    "            rescale=rescale,\n",
    "            data_format=data_format,\n",
    "            dtype=dtype\n",
    "        )\n",
    "    g = idg.flow_from_dataframe(\n",
    "        dataframe=dataframe,\n",
    "        x_col=x_col,\n",
    "        y_col=y_col,\n",
    "        directory=directory,\n",
    "        target_size=target_size,\n",
    "        color_mode=color_mode,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        class_mode=class_mode,\n",
    "        interpolation=interpolation,\n",
    "    )\n",
    "    while True:\n",
    "        x, y = g.next()\n",
    "        yield [x, y], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "characteristic-cooling",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T05:17:19.246386Z",
     "iopub.status.busy": "2021-05-05T05:17:19.245600Z",
     "iopub.status.idle": "2021-05-05T05:17:19.248296Z",
     "shell.execute_reply": "2021-05-05T05:17:19.247753Z"
    },
    "papermill": {
     "duration": 0.026143,
     "end_time": "2021-05-05T05:17:19.248422",
     "exception": false,
     "start_time": "2021-05-05T05:17:19.222279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArcMarginProduct(keras.layers.Layer):\n",
    "    '''\n",
    "    Implements large margin arc distance.\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/pdf/1801.07698.pdf\n",
    "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
    "            blob/master/src/modeling/metric_learning.py\n",
    "    '''\n",
    "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
    "                 ls_eps=0.0, **kwargs):\n",
    "\n",
    "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = tf.math.cos(m)\n",
    "        self.sin_m = tf.math.sin(m)\n",
    "        self.th = tf.math.cos(math.pi - m)\n",
    "        self.mm = tf.math.sin(math.pi - m) * m\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_classes': self.n_classes,\n",
    "            's': self.s,\n",
    "            'm': self.m,\n",
    "            'ls_eps': self.ls_eps,\n",
    "            'easy_margin': self.easy_margin,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcMarginProduct, self).build(input_shape[0])\n",
    "\n",
    "        self.W = self.add_weight(\n",
    "            name='W',\n",
    "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            dtype='float32',\n",
    "            trainable=True,\n",
    "            regularizer=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X, y = inputs\n",
    "        y = tf.cast(y, dtype=tf.int32)\n",
    "        cosine = tf.matmul(\n",
    "            tf.math.l2_normalize(X, axis=1),\n",
    "            tf.math.l2_normalize(self.W, axis=0)\n",
    "        )\n",
    "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = tf.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        one_hot = tf.cast(\n",
    "            tf.one_hot(y, depth=self.n_classes),\n",
    "            dtype=cosine.dtype\n",
    "        )\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "advanced-participant",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T05:17:19.277820Z",
     "iopub.status.busy": "2021-05-05T05:17:19.277029Z",
     "iopub.status.idle": "2021-05-05T05:17:19.280048Z",
     "shell.execute_reply": "2021-05-05T05:17:19.279637Z"
    },
    "papermill": {
     "duration": 0.021619,
     "end_time": "2021-05-05T05:17:19.280145",
     "exception": false,
     "start_time": "2021-05-05T05:17:19.258526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _model(\n",
    "    pretrained,\n",
    "    n_classes: int,\n",
    "    lr: float,\n",
    "    input_shape: Tuple[int, int, int],\n",
    "    dtype=np.float32\n",
    "):\n",
    "    #pretrained.trainable = False\n",
    "    #kernel_initializer = keras.initializers.he_normal()\n",
    "    #kernel_regularizer = keras.regularizers.l2(0.01)\n",
    "    image_input = keras.layers.Input(shape=input_shape, name=\"image_input\")\n",
    "    label_input = keras.layers.Input(shape=(), name=\"label_input\")\n",
    "    x = pretrained(image_input)\n",
    "    x = keras.layers.LayerNormalization(name=\"embedding_output\")(x)\n",
    "    #x = keras.layers.Dense(pretrained.output_shape[1], activation=\"relu\")(x)\n",
    "    #x = keras.layers.LayerNormalization(name=\"embedding_output\")(x)\n",
    "    x = ArcMarginProduct(\n",
    "        n_classes=n_classes, \n",
    "        s=30, \n",
    "        m=0.5, \n",
    "        name='head/arc_margin', \n",
    "        dtype=dtype\n",
    "    )([x, label_input])\n",
    "    output = tf.keras.layers.Softmax(dtype=dtype)(x)\n",
    "    model = tf.keras.models.Model(inputs = [image_input, label_input], outputs = [output])\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy()\n",
    "    sca = keras.metrics.SparseCategoricalAccuracy()\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[sca])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adjustable-rouge",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T05:17:19.305480Z",
     "iopub.status.busy": "2021-05-05T05:17:19.304762Z",
     "iopub.status.idle": "2021-05-05T05:17:29.171569Z",
     "shell.execute_reply": "2021-05-05T05:17:29.172125Z"
    },
    "papermill": {
     "duration": 9.881893,
     "end_time": "2021-05-05T05:17:29.172335",
     "exception": false,
     "start_time": "2021-05-05T05:17:19.290442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "Layer (type)                                     Output Shape                     Param #           Connected to                                      \n",
      "======================================================================================================================================================\n",
      "image_input (InputLayer)                         [(None, 224, 224, 3)]            0                                                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "efficientnetb0 (Functional)                      (None, 1280)                     4049571           image_input[0][0]                                 \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "embedding_output (LayerNormalization)            (None, 1280)                     2560              efficientnetb0[0][0]                              \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "label_input (InputLayer)                         [(None,)]                        0                                                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "head/arc_margin (ArcMarginProduct)               (None, 11014)                    14097920          embedding_output[0][0]                            \n",
      "                                                                                                    label_input[0][0]                                 \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "softmax (Softmax)                                (None, 11014)                    0                 head/arc_margin[0][0]                             \n",
      "======================================================================================================================================================\n",
      "Total params: 18,150,051\n",
      "Trainable params: 18,108,028\n",
      "Non-trainable params: 42,023\n",
      "______________________________________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained = mylib.efficient_net(\n",
    "    variant=MODEL,\n",
    "    pooling=\"avg\",\n",
    "    directory=f\"{RESOURCE_DIR}/pretrained/efficientnet\",\n",
    ")\n",
    "model = _model(\n",
    "    pretrained=pretrained,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    n_classes=n_classes,\n",
    "    lr=1e-3,\n",
    ")\n",
    "model.summary(line_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hollow-campbell",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T05:17:29.198910Z",
     "iopub.status.busy": "2021-05-05T05:17:29.198409Z",
     "iopub.status.idle": "2021-05-05T05:17:29.201935Z",
     "shell.execute_reply": "2021-05-05T05:17:29.202355Z"
    },
    "papermill": {
     "duration": 0.018725,
     "end_time": "2021-05-05T05:17:29.202493",
     "exception": false,
     "start_time": "2021-05-05T05:17:29.183768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _callbacks(patience: int, directory: str):\n",
    "    return [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=patience, verbose=1\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=f\"{directory}/model.h5\",\n",
    "            monitor=\"val_loss\",\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "technological-proceeding",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T05:17:29.237688Z",
     "iopub.status.busy": "2021-05-05T05:17:29.231078Z",
     "iopub.status.idle": "2021-05-05T05:17:29.240170Z",
     "shell.execute_reply": "2021-05-05T05:17:29.239767Z"
    },
    "papermill": {
     "duration": 0.027277,
     "end_time": "2021-05-05T05:17:29.240294",
     "exception": false,
     "start_time": "2021-05-05T05:17:29.213017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyObjective:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        splitter,\n",
    "        groups,\n",
    "        epochs: int,\n",
    "        batch_size: int,\n",
    "        patience: int,\n",
    "        job_dir: str,\n",
    "        lr: Tuple[float, float],\n",
    "        n_classes: int,\n",
    "        input_shape: Tuple[int, int, int],\n",
    "    ):\n",
    "        self.df = df\n",
    "        self.splitter = splitter\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.patience = patience\n",
    "        self.job_dir = job_dir\n",
    "        self.groups = groups\n",
    "        self.lr = lr\n",
    "        self.n_classes = n_classes\n",
    "        self.input_shape = input_shape\n",
    "        self.history: List[Dict[str, Union[str, int, float]]] = []\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        hist = {\n",
    "            \"trial_id\": trial.number,\n",
    "            \"learning_rate\": trial.suggest_loguniform(\n",
    "                \"learning_rate\", self.lr[0], self.lr[1]\n",
    "            ),\n",
    "        }\n",
    "        scores = []\n",
    "        for fold, (ti, vi) in enumerate(\n",
    "            self.splitter.split(self.df.index, None, groups=self.groups)\n",
    "        ):\n",
    "            train_gen = _data_gen(\n",
    "                dataframe=self.df.iloc[ti],\n",
    "                directory=f\"{DATA}/train_images\",\n",
    "                target_size=self.input_shape[:2],\n",
    "                batch_size=self.batch_size,\n",
    "                mode=\"train\"\n",
    "            )\n",
    "            val_gen = _data_gen(\n",
    "                dataframe=self.df.iloc[vi],\n",
    "                directory=f\"{DATA}/train_images\",\n",
    "                target_size=self.input_shape[:2],\n",
    "                batch_size=self.batch_size,\n",
    "                mode=\"val\"\n",
    "            )\n",
    "            pretrained = mylib.efficient_net(\n",
    "                variant=MODEL,\n",
    "                pooling=\"avg\",\n",
    "                directory=f\"{RESOURCE_DIR}/pretrained/efficientnet\",\n",
    "            )\n",
    "            model = _model(\n",
    "                pretrained=pretrained,\n",
    "                input_shape=self.input_shape,\n",
    "                n_classes=self.n_classes,\n",
    "                lr=hist[\"learning_rate\"],\n",
    "            )\n",
    "            #model.summary()\n",
    "            directory = f\"{self.job_dir}/trial_{hist['trial_id']}/fold_{fold}\"\n",
    "            history = model.fit(\n",
    "                train_gen,\n",
    "                epochs=self.epochs,\n",
    "                steps_per_epoch=len(ti) / self.batch_size + 1,\n",
    "                validation_steps=len(vi) / self.batch_size + 1,\n",
    "                validation_data=val_gen,\n",
    "                callbacks=_callbacks(self.patience, directory=directory),\n",
    "                verbose=1\n",
    "            )\n",
    "            #y_pred = model.predict(x_val, batch_size=self.batch_size)\n",
    "            #score = metrics.mean_squared_error(y_val, y_pred, squared=False)\n",
    "            #print(repr(history.history))\n",
    "            score = history.history[\"val_sparse_categorical_accuracy\"][-1]\n",
    "            #log.info(f\"score={score:.4f}, fold={fold}, trial={hist['trial_id']}\")\n",
    "            print(f\"score={score:.4f}, fold={fold}, trial={hist['trial_id']}\")\n",
    "            hist[f\"fold_{fold}_score\"] = score\n",
    "            scores.append(score)\n",
    "            del model\n",
    "            gc.collect()\n",
    "        hist[\"score_mean\"] = np.mean(scores)\n",
    "        hist[\"score_std\"] = np.std(scores)\n",
    "        hist[\"score_worst\"] = max(scores)\n",
    "        self.history.append(hist)\n",
    "        return hist[\"score_worst\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mathematical-basics",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T05:17:29.266776Z",
     "iopub.status.busy": "2021-05-05T05:17:29.266049Z",
     "iopub.status.idle": "2021-05-05T08:51:41.990981Z",
     "shell.execute_reply": "2021-05-05T08:51:41.989634Z"
    },
    "papermill": {
     "duration": 12852.740277,
     "end_time": "2021-05-05T08:51:41.991110",
     "exception": false,
     "start_time": "2021-05-05T05:17:29.250833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-05 05:17:29,264]\u001b[0m A new study created in memory with name: no-name-a8f7a48f-dce0-49e7-a333-cadc314a8e3a\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27399 validated image filenames.\n",
      "Epoch 1/40\n",
      "858/857 [==============================] - ETA: 0s - loss: 21.6573 - sparse_categorical_accuracy: 0.0093Found 6851 validated image filenames.\n",
      "857/857 [==============================] - 448s 509ms/step - loss: 21.6552 - sparse_categorical_accuracy: 0.0093 - val_loss: 24.7003 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 24.70026, saving model to /kaggle/working/trial_0/fold_0/model.h5\n",
      "Epoch 2/40\n",
      "857/857 [==============================] - 347s 405ms/step - loss: 12.0256 - sparse_categorical_accuracy: 0.1451 - val_loss: 28.0634 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 24.70026\n",
      "Epoch 3/40\n",
      "857/857 [==============================] - 346s 404ms/step - loss: 6.7674 - sparse_categorical_accuracy: 0.3245 - val_loss: 27.9502 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 24.70026\n",
      "Epoch 4/40\n",
      "857/857 [==============================] - 351s 410ms/step - loss: 4.3531 - sparse_categorical_accuracy: 0.4913 - val_loss: 28.7228 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 24.70026\n",
      "Epoch 5/40\n",
      "857/857 [==============================] - 352s 411ms/step - loss: 3.1566 - sparse_categorical_accuracy: 0.5934 - val_loss: 27.5346 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 24.70026\n",
      "Epoch 6/40\n",
      "857/857 [==============================] - 352s 411ms/step - loss: 2.3174 - sparse_categorical_accuracy: 0.6847 - val_loss: 28.1156 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 24.70026\n",
      "Epoch 7/40\n",
      "857/857 [==============================] - 349s 408ms/step - loss: 1.7547 - sparse_categorical_accuracy: 0.7616 - val_loss: 26.7221 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 24.70026\n",
      "Epoch 00007: early stopping\n",
      "score=0.0000, fold=0, trial=0\n",
      "Found 27401 validated image filenames.\n",
      "Epoch 1/40\n",
      "858/857 [==============================] - ETA: 0s - loss: 21.7402 - sparse_categorical_accuracy: 0.0085Found 6849 validated image filenames.\n",
      "857/857 [==============================] - 367s 419ms/step - loss: 21.7381 - sparse_categorical_accuracy: 0.0085 - val_loss: 24.5112 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 24.51124, saving model to /kaggle/working/trial_0/fold_1/model.h5\n",
      "Epoch 2/40\n",
      "857/857 [==============================] - 351s 410ms/step - loss: 12.1769 - sparse_categorical_accuracy: 0.1396 - val_loss: 28.0926 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 24.51124\n",
      "Epoch 3/40\n",
      "857/857 [==============================] - 352s 411ms/step - loss: 6.8748 - sparse_categorical_accuracy: 0.3244 - val_loss: 28.1469 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 24.51124\n",
      "Epoch 4/40\n",
      "857/857 [==============================] - 361s 422ms/step - loss: 4.4395 - sparse_categorical_accuracy: 0.4830 - val_loss: 28.2307 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 24.51124\n",
      "Epoch 5/40\n",
      "857/857 [==============================] - 358s 418ms/step - loss: 3.1965 - sparse_categorical_accuracy: 0.5882 - val_loss: 28.0912 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 24.51124\n",
      "Epoch 6/40\n",
      "857/857 [==============================] - 360s 420ms/step - loss: 2.3779 - sparse_categorical_accuracy: 0.6774 - val_loss: 29.6783 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 24.51124\n",
      "Epoch 7/40\n",
      "857/857 [==============================] - 362s 423ms/step - loss: 1.8567 - sparse_categorical_accuracy: 0.7499 - val_loss: 27.4767 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 24.51124\n",
      "Epoch 00007: early stopping\n",
      "score=0.0000, fold=1, trial=0\n",
      "Found 27400 validated image filenames.\n",
      "Epoch 1/40\n",
      "858/857 [==============================] - ETA: 0s - loss: 21.6485 - sparse_categorical_accuracy: 0.0092Found 6850 validated image filenames.\n",
      "857/857 [==============================] - 375s 428ms/step - loss: 21.6464 - sparse_categorical_accuracy: 0.0092 - val_loss: 24.7706 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 24.77060, saving model to /kaggle/working/trial_0/fold_2/model.h5\n",
      "Epoch 2/40\n",
      "857/857 [==============================] - 360s 420ms/step - loss: 11.9535 - sparse_categorical_accuracy: 0.1426 - val_loss: 27.2550 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 24.77060\n",
      "Epoch 3/40\n",
      "857/857 [==============================] - 362s 422ms/step - loss: 6.7472 - sparse_categorical_accuracy: 0.3217 - val_loss: 28.0595 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 24.77060\n",
      "Epoch 4/40\n",
      "857/857 [==============================] - 362s 423ms/step - loss: 4.4040 - sparse_categorical_accuracy: 0.4841 - val_loss: 28.9172 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 24.77060\n",
      "Epoch 5/40\n",
      "857/857 [==============================] - 363s 424ms/step - loss: 3.0849 - sparse_categorical_accuracy: 0.6026 - val_loss: 27.4881 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 24.77060\n",
      "Epoch 6/40\n",
      "857/857 [==============================] - 367s 428ms/step - loss: 2.2898 - sparse_categorical_accuracy: 0.6872 - val_loss: 26.4685 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 24.77060\n",
      "Epoch 7/40\n",
      "857/857 [==============================] - 369s 431ms/step - loss: 1.7787 - sparse_categorical_accuracy: 0.7614 - val_loss: 27.1117 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 24.77060\n",
      "Epoch 00007: early stopping\n",
      "score=0.0000, fold=2, trial=0\n",
      "Found 27400 validated image filenames.\n",
      "Epoch 1/40\n",
      "858/857 [==============================] - ETA: 0s - loss: 21.6109 - sparse_categorical_accuracy: 0.0104Found 6850 validated image filenames.\n",
      "857/857 [==============================] - 378s 431ms/step - loss: 21.6089 - sparse_categorical_accuracy: 0.0104 - val_loss: 24.2373 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 24.23734, saving model to /kaggle/working/trial_0/fold_3/model.h5\n",
      "Epoch 2/40\n",
      "857/857 [==============================] - 365s 426ms/step - loss: 11.9464 - sparse_categorical_accuracy: 0.1453 - val_loss: 27.4309 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 24.23734\n",
      "Epoch 3/40\n",
      "857/857 [==============================] - 366s 427ms/step - loss: 6.8263 - sparse_categorical_accuracy: 0.3202 - val_loss: 28.7243 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 24.23734\n",
      "Epoch 4/40\n",
      "857/857 [==============================] - 364s 425ms/step - loss: 4.3348 - sparse_categorical_accuracy: 0.4944 - val_loss: 27.4156 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 24.23734\n",
      "Epoch 5/40\n",
      "857/857 [==============================] - 364s 425ms/step - loss: 3.1218 - sparse_categorical_accuracy: 0.5962 - val_loss: 27.1809 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 24.23734\n",
      "Epoch 6/40\n",
      "857/857 [==============================] - 364s 425ms/step - loss: 2.2900 - sparse_categorical_accuracy: 0.6903 - val_loss: 27.1380 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 24.23734\n",
      "Epoch 7/40\n",
      "857/857 [==============================] - 367s 429ms/step - loss: 1.8179 - sparse_categorical_accuracy: 0.7552 - val_loss: 27.2517 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 24.23734\n",
      "Epoch 00007: early stopping\n",
      "score=0.0000, fold=3, trial=0\n",
      "Found 27400 validated image filenames.\n",
      "Epoch 1/40\n",
      "858/857 [==============================] - ETA: 0s - loss: 21.7244 - sparse_categorical_accuracy: 0.0093Found 6850 validated image filenames.\n",
      "857/857 [==============================] - 376s 429ms/step - loss: 21.7223 - sparse_categorical_accuracy: 0.0093 - val_loss: 25.0635 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 25.06347, saving model to /kaggle/working/trial_0/fold_4/model.h5\n",
      "Epoch 2/40\n",
      "857/857 [==============================] - 373s 436ms/step - loss: 12.1204 - sparse_categorical_accuracy: 0.1387 - val_loss: 25.2540 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 25.06347\n",
      "Epoch 3/40\n",
      "857/857 [==============================] - 374s 436ms/step - loss: 6.8787 - sparse_categorical_accuracy: 0.3148 - val_loss: 28.2615 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 25.06347\n",
      "Epoch 4/40\n",
      "857/857 [==============================] - 375s 438ms/step - loss: 4.4129 - sparse_categorical_accuracy: 0.4891 - val_loss: 27.3816 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 25.06347\n",
      "Epoch 5/40\n",
      "857/857 [==============================] - 373s 436ms/step - loss: 3.1741 - sparse_categorical_accuracy: 0.5873 - val_loss: 27.9245 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 25.06347\n",
      "Epoch 6/40\n",
      "857/857 [==============================] - 374s 437ms/step - loss: 2.3148 - sparse_categorical_accuracy: 0.6870 - val_loss: 27.1394 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 25.06347\n",
      "Epoch 7/40\n",
      "857/857 [==============================] - 358s 417ms/step - loss: 1.8459 - sparse_categorical_accuracy: 0.7566 - val_loss: 26.7179 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 25.06347\n",
      "Epoch 00007: early stopping\n",
      "score=0.0000, fold=4, trial=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-05-05 08:51:34,002]\u001b[0m Trial 0 finished with value: 0.0 and parameters: {'learning_rate': 0.001}. Best is trial 0 with value: 0.0.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "obj = MyObjective(\n",
    "    df=train,\n",
    "    splitter=sklearn.model_selection.GroupKFold(n_splits=5),\n",
    "    groups=train[\"label_group\"],\n",
    "    epochs=40,\n",
    "    batch_size=32,  # B3: OOM if batch size > 128 \n",
    "    patience=6,\n",
    "    job_dir=OUTPUT,\n",
    "    lr=(1e-3, 1e-3),\n",
    "    n_classes=n_classes,\n",
    "    input_shape=INPUT_SHAPE,\n",
    ")\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(obj, n_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "australian-midnight",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T08:51:58.188317Z",
     "iopub.status.busy": "2021-05-05T08:51:58.187615Z",
     "iopub.status.idle": "2021-05-05T08:51:58.350574Z",
     "shell.execute_reply": "2021-05-05T08:51:58.351040Z"
    },
    "papermill": {
     "duration": 8.448091,
     "end_time": "2021-05-05T08:51:58.351213",
     "exception": false,
     "start_time": "2021-05-05T08:51:49.903122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_id</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>fold_0_score</th>\n",
       "      <th>fold_1_score</th>\n",
       "      <th>fold_2_score</th>\n",
       "      <th>fold_3_score</th>\n",
       "      <th>fold_4_score</th>\n",
       "      <th>score_mean</th>\n",
       "      <th>score_std</th>\n",
       "      <th>score_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trial_id  learning_rate  fold_0_score  fold_1_score  fold_2_score  \\\n",
       "0         0          0.001           0.0           0.0           0.0   \n",
       "\n",
       "   fold_3_score  fold_4_score  score_mean  score_std  score_worst  \n",
       "0           0.0           0.0         0.0        0.0          0.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = pd.DataFrame.from_records(obj.history)\n",
    "history.sort_values(\"score_worst\", ascending=False, inplace=True, ignore_index=True)\n",
    "history.to_csv(f\"{OUTPUT}/cv.csv\", index=False)\n",
    "history.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12908.671896,
   "end_time": "2021-05-05T08:52:10.268577",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-05T05:17:01.596681",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
