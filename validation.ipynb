{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c47f2f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import configparser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import py\n",
    "import mylib\n",
    "import cv2 as cv\n",
    "import pytesseract\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, List, Dict, Set, Tuple\n",
    "from scml.nlp import strip_punctuation, to_ascii_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63c34ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resolution=300\n"
     ]
    }
   ],
   "source": [
    "IMAGE = True\n",
    "TITLE = True\n",
    "PHASH = True\n",
    "OCR = False\n",
    "MODEL = 'efficientnetb3'\n",
    "pd.set_option(\"use_inf_as_na\", True)\n",
    "pd.set_option(\"display.max_columns\", 9999)\n",
    "pd.set_option(\"display.max_rows\", 9999)\n",
    "pd.set_option('max_colwidth', 9999)\n",
    "#os.environ[\"OMP_THREAD_LIMIT\"] = \"1\"\n",
    "CONF = configparser.ConfigParser()\n",
    "CONF.read(\"app.ini\")\n",
    "resolution = int(CONF[MODEL][\"resolution\"])\n",
    "print(f\"resolution={resolution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd64e572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34250 entries, 0 to 34249\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   posting_id   34250 non-null  object\n",
      " 1   image        34250 non-null  object\n",
      " 2   image_phash  34250 non-null  object\n",
      " 3   title        34250 non-null  object\n",
      " 4   label_group  34250 non-null  int64 \n",
      " 5   target       34250 non-null  object\n",
      " 6   image_path   34250 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"input/train.csv\", engine=\"c\", low_memory=False)\n",
    "train[\"target\"] = mylib.target_label(train)\n",
    "train[\"image_path\"] = \"input/train_images/\" + train[\"image\"]\n",
    "posting_ids = train[\"posting_id\"].tolist()\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57e5e12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.9 s, sys: 281 ms, total: 34.2 s\n",
      "Wall time: 34.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# required for post-processing\n",
    "train[\"title_p\"] = train.apply(mylib.preprocess(\"title\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b796618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34250it [02:04, 275.74it/s]\n"
     ]
    }
   ],
   "source": [
    "imap = {}\n",
    "for t in tqdm(train.itertuples()):\n",
    "    pid = getattr(t, \"posting_id\")\n",
    "    title = getattr(t, \"title_p\")\n",
    "    imap[pid] = mylib.extract(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07f4909",
   "metadata": {},
   "source": [
    "# PHash\n",
    "th=.25, f1=.586 | th=.30, f1=.586 | th=.35, f1=.587 | th=.40, f1=.583"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "356175a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.6 s, sys: 51.1 s, total: 1min 47s\n",
      "Wall time: 54.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if PHASH:\n",
    "    train[\"phash_matches\"] = mylib.phash_matches(train, threshold=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38742f1",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db16228e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min, sys: 33.5 s, total: 21min 33s\n",
      "Wall time: 4min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if TITLE:\n",
    "    st_name = \"stsb-distilbert-base\"\n",
    "    #st_name = \"paraphrase-distilroberta-base-v1\"\n",
    "    #st_name = \"paraphrase-xlm-r-multilingual-v1\"\n",
    "    train[\"title_matches\"] = mylib.sbert_matches(\n",
    "        model_path=f\"pretrained/sentence-transformers/{st_name}\",\n",
    "        sentences=train[\"title_p\"].tolist(),\n",
    "        posting_ids=posting_ids,\n",
    "        threshold=0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ec52b",
   "metadata": {},
   "source": [
    "# Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69ab20a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     [(None, 300, 300, 3)]     0         \n",
      "_________________________________________________________________\n",
      "efficientnetb3 (Functional)  (None, 1536)              10783535  \n",
      "_________________________________________________________________\n",
      "layer_normalization_1 (Layer (None, 1536)              3072      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1536)              2360832   \n",
      "_________________________________________________________________\n",
      "embedding_output (LayerNorma (None, 1536)              3072      \n",
      "=================================================================\n",
      "Total params: 13,150,511\n",
      "Trainable params: 2,366,976\n",
      "Non-trainable params: 10,783,535\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if IMAGE:\n",
    "    model_dir = \"models/eb3_arc_20210510_1800\"\n",
    "    m0 = keras.models.load_model(f\"{model_dir}/trial_0/model.h5\")\n",
    "    m0 = keras.models.Model(inputs=m0.input[0], outputs=m0.get_layer(\"embedding_output\").output)\n",
    "    m0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f93e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34250 validated image filenames.\n",
      "30/34 [=========================>....] - ETA: 22:28"
     ]
    }
   ],
   "source": [
    "if IMAGE:\n",
    "    idg = keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        data_format=\"channels_last\",\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    data = idg.flow_from_dataframe(\n",
    "        dataframe=train,\n",
    "        x_col=\"image\",\n",
    "        y_col=\"label_group\",\n",
    "        directory=\"input/train_images\",\n",
    "        target_size=(resolution, resolution),\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=1024,\n",
    "        shuffle=False,\n",
    "        class_mode=\"raw\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    y0 = m0.predict(data, verbose=1)\n",
    "    #y1 = m1.predict(data, verbose=1)\n",
    "    #y2 = m2.predict(data, verbose=1)\n",
    "    #y3 = m3.predict(data, verbose=1)\n",
    "    #y4 = m4.predict(data, verbose=1)\n",
    "    #assert y0.shape == y1.shape == y2.shape == y3.shape == y4.shape\n",
    "    #print(f\"y0.shape={y0.shape}\")\n",
    "    em = y0.astype(np.float32)\n",
    "    print(f\"em.shape={em.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d0ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = []\n",
    "#for i in range(len(y0)):\n",
    "    #a = np.vstack((y0[i], y1[i], y2[i], y3[i], y4[i]))\n",
    "    #a = np.vstack((y0[i], y1[i]))\n",
    "    #m = np.mean(a, axis=0)\n",
    "    #res.append(m)\n",
    "#em = np.array(res, dtype=np.float32)\n",
    "#assert y0.shape == em.shape\n",
    "#print(f\"em.shape={em.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f2dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if IMAGE:\n",
    "    threshold = 1e-4\n",
    "    nn = NearestNeighbors(\n",
    "        n_neighbors=min(49, len(posting_ids) - 1), metric=\"euclidean\", n_jobs=-1\n",
    "    )\n",
    "    nn.fit(em)\n",
    "    distances, indices = nn.kneighbors()\n",
    "    res: List[List[str]] = [[] for _ in range(len(indices))]\n",
    "    for i in range(len(indices)):\n",
    "        for j in range(len(indices[0])):\n",
    "            if distances[i][j] > threshold:\n",
    "                break\n",
    "            res[i].append(posting_ids[indices[i][j]])\n",
    "    train[\"image_matches\"] = res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15340b51",
   "metadata": {},
   "source": [
    "# OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be017000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def erode_dilate(img):\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    img = cv.erode(img, kernel, iterations=1)\n",
    "    img = cv.dilate(img, kernel, iterations=1)\n",
    "    return img\n",
    "\n",
    "\n",
    "def image_to_text(img_path, mode: str, timeout: float, neighbours: int=41, psm: int=3) -> Optional[str]:\n",
    "    config = f\"--psm {psm}\"\n",
    "    s1, s2 = None, None\n",
    "    img = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "    #img = cv.resize(img, None, fx=0.5, fy=0.5, interpolation=cv.INTER_AREA)\n",
    "    img = cv.medianBlur(img, 3)\n",
    "    if mode == \"binary_inverted\" or mode == \"binary\":\n",
    "        th = cv.adaptiveThreshold(img, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY, neighbours, 2)\n",
    "        th = erode_dilate(th)\n",
    "        try:\n",
    "            s1 = pytesseract.image_to_string(th, timeout=timeout, config=config)\n",
    "        except:\n",
    "            s1 = None\n",
    "    if mode == \"binary_inverted\" or mode == \"inverted\":\n",
    "        th = cv.adaptiveThreshold(img, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY_INV, neighbours, 2)\n",
    "        th = erode_dilate(th)\n",
    "        try:\n",
    "            s2 = pytesseract.image_to_string(th, timeout=timeout, config=config)\n",
    "        except:\n",
    "            s2 = None\n",
    "    if s1 is None and s2 is None:\n",
    "        return None\n",
    "    tokens = []\n",
    "    if s1 is not None:\n",
    "        s1 = to_ascii_str(s1)\n",
    "        s1 = strip_punctuation(s1)\n",
    "        tokens += s1.split()\n",
    "    if s2 is not None:\n",
    "        s2 = to_ascii_str(s2)\n",
    "        s2 = strip_punctuation(s2)\n",
    "        tokens += s2.split()\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be95f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OCR:\n",
    "    res = []\n",
    "    n_timeout = 0\n",
    "    for t in tqdm(train.itertuples()):\n",
    "        img_path = getattr(t, \"image_path\")\n",
    "        s = image_to_text(img_path, mode=\"inverted\", timeout=0.4, neighbours=41, psm=11)\n",
    "        if s is None:\n",
    "            s = \"\"\n",
    "            n_timeout += 1\n",
    "        res.append(s)\n",
    "    print(f\"n_timeout={n_timeout}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d6e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OCR:\n",
    "    train[\"itext\"] = res\n",
    "    train[\"text\"] = train[\"title\"] + \" \" + train[\"itext\"]\n",
    "    cols = [\"text\", \"itext\", \"title\"]\n",
    "    train[cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baed52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if OCR:\n",
    "    train[\"text_p\"] = train.apply(mylib.preprocess(\"text\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ba74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OCR:\n",
    "    st_name = \"stsb-distilbert-base\"\n",
    "    #st_name = \"paraphrase-distilroberta-base-v1\"\n",
    "    #st_name = \"paraphrase-xlm-r-multilingual-v1\"\n",
    "    train[\"text_matches\"] = mylib.sbert_matches(\n",
    "        model_path=f\"pretrained/sentence-transformers/{st_name}\",\n",
    "        sentences=train[\"text_p\"].tolist(),\n",
    "        posting_ids=posting_ids,\n",
    "        threshold=0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f9ff9",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b834c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = []\n",
    "if IMAGE:\n",
    "    fs.append(\"image_matches\")\n",
    "if TITLE:\n",
    "    fs.append(\"title_matches\")\n",
    "if PHASH:\n",
    "    fs.append(\"phash_matches\")\n",
    "if OCR:\n",
    "    fs.append(\"text_matches\")\n",
    "train[\"matches\"] = train.apply(mylib.combine_as_list(\n",
    "    fs,\n",
    "    imap=imap,\n",
    "    brand_threshold=0.5,\n",
    "    measurement_threshold=0.5,\n",
    "), axis=1)\n",
    "train[\"f1\"] = train.apply(mylib.metric_per_row(\"matches\"), axis=1)\n",
    "print(f\"Combined score={train.f1.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c679a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [\n",
    "    {\n",
    "        \"score\": 0.654,\n",
    "        \"phash_threshold\": 0.3,\n",
    "        \"title_threshold\": 0.5,\n",
    "        \"image_threshold\": 1e-6,\n",
    "        \"image_pretrained\": \"enb3\",\n",
    "        \"brand_theshold\": 0.3,\n",
    "        \"measurement_threshold\": 0.3,\n",
    "        \"text_threshold\": None,\n",
    "        \"ocr_threshold\": None,\n",
    "        \"ocr_timeout\": None,\n",
    "        \"ocr_neighbours\": None,\n",
    "        \"ocr_psm\": None\n",
    "    },\n",
    "    {\n",
    "        \"score\": 0.654,\n",
    "        \"phash_threshold\": 0.3,\n",
    "        \"title_threshold\": 0.5,\n",
    "        \"image_threshold\": 1e-5,\n",
    "        \"image_pretrained\": \"enb3\",\n",
    "        \"brand_theshold\": 0.3,\n",
    "        \"measurement_threshold\": 0.3,\n",
    "        \"text_threshold\": None,\n",
    "        \"ocr_threshold\": None,\n",
    "        \"ocr_timeout\": None,\n",
    "        \"ocr_neighbours\": None,\n",
    "        \"ocr_psm\": None\n",
    "    },\n",
    "    {\n",
    "        \"score\": 0.654,\n",
    "        \"phash_threshold\": 0.3,\n",
    "        \"title_threshold\": 0.5,\n",
    "        \"image_threshold\": 1e-4,\n",
    "        \"image_pretrained\": \"enb3\",\n",
    "        \"brand_theshold\": 0.3,\n",
    "        \"measurement_threshold\": 0.3,\n",
    "        \"text_threshold\": None,\n",
    "        \"ocr_threshold\": None,\n",
    "        \"ocr_timeout\": None,\n",
    "        \"ocr_neighbours\": None,\n",
    "        \"ocr_psm\": None\n",
    "    },\n",
    "    {\n",
    "        \"score\": 0.645,\n",
    "        \"phash_threshold\": 0.3,\n",
    "        \"title_threshold\": 0.5,\n",
    "        \"image_threshold\": 1e-3,\n",
    "        \"image_pretrained\": \"enb3\",\n",
    "        \"brand_theshold\": 0.3,\n",
    "        \"measurement_threshold\": 0.3,\n",
    "        \"text_threshold\": None,\n",
    "        \"ocr_threshold\": None,\n",
    "        \"ocr_timeout\": None,\n",
    "        \"ocr_neighbours\": None,\n",
    "        \"ocr_psm\": None\n",
    "    },\n",
    "    {\n",
    "        \"score\": 0.656,\n",
    "        \"phash_threshold\": 0.3,\n",
    "        \"title_threshold\": 0.5,\n",
    "        \"image_threshold\": 5e-3,\n",
    "        \"image_pretrained\": \"enb3\",\n",
    "        \"text_threshold\": None,\n",
    "        \"ocr_threshold\": None,\n",
    "        \"ocr_timeout\": None,\n",
    "        \"ocr_neighbours\": None,\n",
    "        \"ocr_psm\": None\n",
    "    },\n",
    "    {\n",
    "        \"score\": 0.522,\n",
    "        \"phash_threshold\": None,\n",
    "        \"title_threshold\": None,\n",
    "        \"image_threshold\": 5e-3,\n",
    "        \"image_pretrained\": \"enb3\",\n",
    "        \"text_threshold\": None,\n",
    "        \"ocr_threshold\": None,\n",
    "        \"ocr_timeout\": None,\n",
    "        \"ocr_neighbours\": None,\n",
    "        \"ocr_psm\": None\n",
    "    },\n",
    "    {\n",
    "        \"score\": 0.473,\n",
    "        \"phash_threshold\": None,\n",
    "        \"title_threshold\": None,\n",
    "        \"image_threshold\": 0.01,\n",
    "        \"image_pretrained\": \"enb3\",\n",
    "        \"text_threshold\": None,\n",
    "        \"ocr_threshold\": None,\n",
    "        \"ocr_timeout\": None,\n",
    "        \"ocr_neighbours\": None,\n",
    "        \"ocr_psm\": None\n",
    "    },\n",
    "    {\n",
    "        \"score\": 0.502,\n",
    "        \"phash_threshold\": None,\n",
    "        \"title_threshold\": None,\n",
    "        \"image_threshold\": 1e-3,\n",
    "        \"image_pretrained\": \"enb3\",\n",
    "        \"text_threshold\": None,\n",
    "        \"ocr_threshold\": None,\n",
    "        \"ocr_timeout\": None,\n",
    "        \"ocr_neighbours\": None,\n",
    "        \"ocr_psm\": None\n",
    "    },\n",
    "    {\n",
    "        \"score\": 0.651,\n",
    "        \"phash_threshold\": 0.2,\n",
    "        \"title_threshold\": 0.5,\n",
    "        \"image_threshold\": 1e-4,\n",
    "        \"image_pretrained\": \"enb3\",\n",
    "        \"text_threshold\": None,\n",
    "        \"ocr_threshold\": None,\n",
    "        \"ocr_timeout\": None,\n",
    "        \"ocr_neighbours\": None,\n",
    "        \"ocr_psm\": None\n",
    "    },\n",
    "    {\n",
    "        \"score\": 0.654,\n",
    "        \"phash_threshold\": 0.2,\n",
    "        \"title_threshold\": 0.5,\n",
    "        \"image_threshold\": 1e-5,\n",
    "        \"image_pretrained\": \"enb3\",\n",
    "        \"text_threshold\": None,\n",
    "        \"ocr_threshold\": None,\n",
    "        \"ocr_timeout\": None,\n",
    "        \"ocr_neighbours\": None,\n",
    "        \"ocr_psm\": None\n",
    "    },\n",
    "    {\n",
    "        \"score\": 0.658,\n",
    "        \"phash_threshold\": 0.3,\n",
    "        \"title_threshold\": 0.5,\n",
    "        \"image_threshold\": 1e-5,\n",
    "        \"image_pretrained\": \"enb3\",\n",
    "        \"text_threshold\": None,\n",
    "        \"ocr_threshold\": None,\n",
    "        \"ocr_timeout\": None,\n",
    "        \"ocr_neighbours\": None,\n",
    "        \"ocr_psm\": None\n",
    "    },\n",
    "    {\n",
    "        \"score\": 0.656,\n",
    "        \"phash_threshold\": 0.3,\n",
    "        \"title_threshold\": 0.5,\n",
    "        \"image_threshold\": 1e-4,\n",
    "        \"image_pretrained\": \"enb3\",\n",
    "        \"text_threshold\": None,\n",
    "        \"ocr_threshold\": None,\n",
    "        \"ocr_timeout\": None,\n",
    "        \"ocr_neighbours\": None,\n",
    "        \"ocr_psm\": None\n",
    "    },\n",
    "    {\n",
    "        \"score\": 0.562,\n",
    "        \"phash_threshold\": 0.3,\n",
    "        \"title_threshold\": 0.5,\n",
    "        \"image_threshold\": 0.001,\n",
    "        \"image_pretrained\": \"enb3\",\n",
    "        \"text_threshold\": None,\n",
    "        \"ocr_threshold\": None,\n",
    "        \"ocr_timeout\": None,\n",
    "        \"ocr_neighbours\": None,\n",
    "        \"ocr_psm\": None\n",
    "    },\n",
    "    {\n",
    "        \"score\": 0.514,\n",
    "        \"phash_threshold\": 0.3,\n",
    "        \"title_threshold\": 0.5,\n",
    "        \"image_threshold\": 0.001,\n",
    "        \"image_pretrained\": \"enb0\",\n",
    "        \"text_threshold\": None,\n",
    "        \"ocr_threshold\": None,\n",
    "        \"ocr_timeout\": None,\n",
    "        \"ocr_neighbours\": None,\n",
    "        \"ocr_psm\": None\n",
    "    },\n",
    "    {\n",
    "        \"score\": 0.498,\n",
    "        \"phash_threshold\": 0.3,\n",
    "        \"title_threshold\": 0.5,\n",
    "        \"image_threshold\": 0.01,\n",
    "        \"image_pretrained\": \"enb0\",\n",
    "        \"text_threshold\": None,\n",
    "        \"ocr_threshold\": None,\n",
    "        \"ocr_timeout\": None,\n",
    "        \"ocr_neighbours\": None,\n",
    "        \"ocr_psm\": None\n",
    "    },\n",
    "    {\n",
    "        \"score\": 0.136,\n",
    "        \"phash_threshold\": 0.3,\n",
    "        \"title_threshold\": 0.5,\n",
    "        \"image_threshold\": 0.05,\n",
    "        \"image_pretrained\": \"enb0\",\n",
    "        \"text_threshold\": None,\n",
    "        \"ocr_threshold\": None,\n",
    "        \"ocr_timeout\": None,\n",
    "        \"ocr_neighbours\": None,\n",
    "        \"ocr_psm\": None\n",
    "    },\n",
    "    {\n",
    "        \"score\": 0.674,\n",
    "        \"phash_threshold\": 0.3,\n",
    "        \"title_threshold\": 0.5,\n",
    "        \"text_threshold\": 0.5,\n",
    "        \"image_threshold\": None,\n",
    "        \"image_pretrained\": None,\n",
    "        \"ocr_threshold\": \"inverted\",\n",
    "        \"ocr_timeout\": 0.4,\n",
    "        \"ocr_neighbours\": 41,\n",
    "        \"ocr_psm\": 11\n",
    "    },\n",
    "    {\n",
    "        \"score\": 0.674,\n",
    "        \"phash_threshold\": 0.3,\n",
    "        \"title_threshold\": 0.5,\n",
    "        \"text_threshold\": 0.5,\n",
    "        \"image_threshold\": None,\n",
    "        \"image_pretrained\": None,\n",
    "        \"ocr_threshold\": \"binary\",\n",
    "        \"ocr_timeout\": 0.4,\n",
    "        \"ocr_neighbours\": 41,\n",
    "        \"ocr_psm\": 11\n",
    "    },\n",
    "    {\n",
    "        \"score\": 0.674,\n",
    "        \"phash_threshold\": 0.3,\n",
    "        \"title_threshold\": 0.5,\n",
    "        \"image_threshold\": None,\n",
    "        \"image_pretrained\": None,\n",
    "        \"text_threshold\": None,\n",
    "        \"ocr_threshold\": None,\n",
    "        \"ocr_timeout\": None,\n",
    "        \"ocr_neighbours\": None,\n",
    "        \"ocr_psm\": None\n",
    "    }\n",
    "]\n",
    "df = pd.DataFrame.from_records(res)\n",
    "df.sort_values(\"score\", ascending=False, inplace=True, ignore_index=True)\n",
    "df.T.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fc8f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"f1\", \"target\", \"matches\"] + fs\n",
    "train[cols].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b51767",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.sort_values(\"f1\", ascending=True, ignore_index=True)\n",
    "df[cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62c873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
